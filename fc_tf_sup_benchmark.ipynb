{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# if working on laptop on local docker:\\ndocker run -p 4242:8888 -v ~/dl_cas/:/notebooks -p 6006:6006 -it oduerr/tf_docker:tf1_py3\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# if working on laptop on local docker:\n",
    "docker run -p 4242:8888 -v ~/dl_cas/:/notebooks -p 6006:6006 -it oduerr/tf_docker:tf1_py3\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# python module imports needed in customized functions:\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "# import customized functions collected in my_fct_for_fc_ladder.py\n",
    "from my_fct_for_fc_ladder_Lilach_vb import convertToOneHot\n",
    "from my_fct_for_fc_ladder_Lilach_vb import my_norm\n",
    "from my_fct_for_fc_ladder_Lilach_vb import my_fc_bn\n",
    "from my_fct_for_fc_ladder_Lilach_vb import my_noise\n",
    "from my_fct_for_fc_ladder_Lilach_vb import my_comb_vanilla\n",
    "from my_fct_for_fc_ladder_Lilach_vb import encoder_prop\n",
    "from my_fct_for_fc_ladder_Lilach_vb import decoder_prop\n",
    "#from my_fct_for_fc_ladder_Lilach_vb import init_weights\n",
    "from my_fct_for_fc_ladder_Lilach_vb import init_weights_v2\n",
    "#from my_fct_for_fc_ladder_Lilach import init_weights_combiner_vanilla\n",
    "#from my_fct_for_fc_ladder_Lilach import reconst_loss\n",
    "\n",
    "#from my_fct_for_fc_ladder_Lilach_debug import decoder_prop_ver ## for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.0.0',\n",
       " sys.version_info(major=3, minor=4, micro=3, releaselevel='final', serial=0))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# additional imports of python modules\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imgplot\n",
    "import time\n",
    "import pandas as pd\n",
    "#tf.set_random_seed(1)\n",
    "#np.random.seed(1)\n",
    "import sys\n",
    "tf.__version__, sys.version_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data read-in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load small external MNIST data set when for working local on windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small data before split X.shape (4000, 784)\n",
      "small data before  y.shape (4000,)\n",
      "small data x_train.shape: (3000, 784)\n",
      "small data y_train.shape: (3000,)\n",
      "small data x_test.shape: (1000, 784)\n",
      "small data y_test.shape: (1000,)\n",
      "num_class: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# upload mnist_4000.pkl.gz which we have used in the DL course to home\n",
    "# To be compatible with python3 and python2\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open('mnist_4000.pkl.gz', 'rb') as f:\n",
    "    if sys.version_info.major > 2:\n",
    "        (X,y) = pickle.load(f, encoding='latin1')\n",
    "    else:\n",
    "        (X,y) = pickle.load(f)\n",
    "PIXELS = len(X[0,0,0,:])\n",
    "\n",
    "# if images are not flatten (like in mnist) we need first to flatten them\n",
    "# now flatten images for fc ladder\n",
    "\n",
    "X = X.reshape([4000, 784])\n",
    "#X = X/255 # is already normalized\n",
    "\n",
    "print(\"small data before split X.shape\", X.shape)\n",
    "print(\"small data before  y.shape\", y.shape) \n",
    "\n",
    "x_train = X[0:3000]\n",
    "y_train = y[0:3000]\n",
    "x_test = X[3000:4000]\n",
    "y_test = y[3000:4000]\n",
    "\n",
    "\n",
    "print(\"small data x_train.shape:\", x_train.shape)\n",
    "print(\"small data y_train.shape:\",y_train.shape)\n",
    "print(\"small data x_test.shape:\",x_test.shape)\n",
    "print(\"small data y_test.shape:\",y_test.shape)\n",
    "\n",
    "num_class= len(np.unique(y))\n",
    "print(\"num_class:\",num_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Or load full MNIST dataset directly from internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom keras.datasets import mnist\\n\\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\\n\\n# if images are not flatten (like in mnist) we need first to flatten them\\n# now flatten images for fc ladder\\n\\nx_train = x_train.reshape(-1,784)\\nx_test = x_test.reshape(-1,784)\\n\\nprint(\"large data x_train.shape:\", x_train.shape)\\nprint(\"large data y_train.shape:\",y_train.shape)\\nprint(\"large data x_test.shape:\",x_test.shape)\\nprint(\"large data x_test.shape:\",y_test.shape)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# if images are not flatten (like in mnist) we need first to flatten them\n",
    "# now flatten images for fc ladder\n",
    "\n",
    "x_train = x_train.reshape(-1,784)\n",
    "x_test = x_test.reshape(-1,784)\n",
    "\n",
    "print(\"large data x_train.shape:\", x_train.shape)\n",
    "print(\"large data y_train.shape:\",y_train.shape)\n",
    "print(\"large data x_test.shape:\",x_test.shape)\n",
    "print(\"large data x_test.shape:\",y_test.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:200].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0042952602"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(x_train[:,200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83271211"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(x_train[:,200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lets construct a fc NN (784->500->50->10) without noise and unsupervised task to get a benchmark for the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# reset the default graph\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define placeholder which we need later to feed in our data:\n",
    "# be sure that input data is normalized\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784], name='x_data')\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# our benchmark model hast 3 hidden layers\n",
    "# x:h0:784 -> h1:500 -> h2:50 -> h3:10 (softmax)\n",
    "\n",
    "# initialization\n",
    "############################################\n",
    "# we use exactly the sampe architecture and initialization procedure like in ladder\n",
    "    \n",
    "# initialization of learnable variables (see custum fct init_weights)\n",
    "# predefine intialization of variables which are updated during trianing:\n",
    "# we use no biases since we do batchnorm (following Martin GÃ¶rner)\n",
    "# Bs are used as offset in batchnorm (no scales in bn since we use ReLu)\n",
    "\n",
    "my_sd = 0.1  # sd for initialization of W with N(0,sd)\n",
    "fan = 10     # for dividing the 1-initialized B\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform(shape=[784, 500],minval=-0.05, maxval=0.05))  \n",
    "Blt1 = tf.Variable(tf.zeros([500]))\n",
    "B1 = tf.Variable(tf.zeros([500]))\n",
    "W2 = tf.Variable(tf.random_uniform(shape=[500, 50],minval=-0.05, maxval=0.05))\n",
    "Blt2 = tf.Variable(tf.zeros([50]))\n",
    "B2 = tf.Variable(tf.zeros([50]))\n",
    "W3 = tf.Variable(tf.random_uniform(shape=[50, 10],minval=-0.05, maxval=0.05))\n",
    "Blt3 = tf.Variable(tf.zeros([10]))\n",
    "B3 = tf.Variable(tf.zeros([10]))\n",
    "#gamma = tf.Variable(tf.random_uniform(shape=[10],minval=-0.05, maxval=0.05)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define the model architecture as encoder in ladder: \n",
    "# TODO maby skip my_fc_bn?\n",
    "\n",
    "x = my_norm(x, \"initial_z_trafo\")\n",
    "\n",
    "hn_lt_1 = tf.matmul(x, W1) + Blt1 \n",
    "hn_bn_1 = my_fc_bn(Ylogits=hn_lt_1, offset=B1, scope=\"bn\")\n",
    "hn_nlt_1 = tf.nn.relu(hn_bn_1)\n",
    " \n",
    "hn_lt_2 = tf.matmul(hn_nlt_1, W2) + Blt2\n",
    "hn_bn_2 = my_fc_bn(Ylogits=hn_lt_2, offset=B2, scope=\"bn\")\n",
    "hn_nlt_2 = tf.nn.relu(hn_bn_2)\n",
    "\n",
    "hn_lt_3 = tf.matmul(hn_nlt_2, W3) + Blt3\n",
    "hn_bn_3 = hn_lt_3 #my_fc_bn(Ylogits=hn_lt_3, offset=B3, scope=\"bn\")\n",
    "out = tf.nn.softmax(hn_bn_3)  # TODO : IS THIS ERROR PRONE? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# supervised loss\n",
    "############################################################################\n",
    "\n",
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), \n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n",
    "# problems with log(0) which is NaN\n",
    "\n",
    "with tf.name_scope(\"loss_supervised\"):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=hn_bn_3, labels=y_true)\n",
    "    # loss from supervised learning:\n",
    "    loss_supervised = tf.reduce_mean(cross_entropy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initialize\"):\n",
    "    init_op = tf.global_variables_initializer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train by using  SGD Optimizer\n",
    "with tf.name_scope(\"train_step\"):\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss_supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding  Variable:0\n",
      "INFO:tensorflow:Summary name Variable:0 is illegal; using Variable_0 instead.\n",
      "Adding  Variable_1:0\n",
      "INFO:tensorflow:Summary name Variable_1:0 is illegal; using Variable_1_0 instead.\n",
      "Adding  Variable_2:0\n",
      "INFO:tensorflow:Summary name Variable_2:0 is illegal; using Variable_2_0 instead.\n",
      "Adding  Variable_3:0\n",
      "INFO:tensorflow:Summary name Variable_3:0 is illegal; using Variable_3_0 instead.\n",
      "Adding  Variable_4:0\n",
      "INFO:tensorflow:Summary name Variable_4:0 is illegal; using Variable_4_0 instead.\n",
      "Adding  Variable_5:0\n",
      "INFO:tensorflow:Summary name Variable_5:0 is illegal; using Variable_5_0 instead.\n",
      "Adding  Variable_6:0\n",
      "INFO:tensorflow:Summary name Variable_6:0 is illegal; using Variable_6_0 instead.\n",
      "Adding  Variable_7:0\n",
      "INFO:tensorflow:Summary name Variable_7:0 is illegal; using Variable_7_0 instead.\n",
      "Adding  Variable_8:0\n",
      "INFO:tensorflow:Summary name Variable_8:0 is illegal; using Variable_8_0 instead.\n"
     ]
    }
   ],
   "source": [
    "#We want to visualize the development of the following variables in tensorboard:\n",
    "for v in tf.trainable_variables():\n",
    "    print(\"Adding \", v.name)\n",
    "    tf.summary.histogram(v.name, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_supervised_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to visualize the development of the loss in tensorboard\n",
    "\n",
    "tf.summary.scalar(\"loss_supervised\", loss_supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! rm -rf /tmp/ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! mkdir /tmp/ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! ls /tmp/ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# collect all summaries for tensorboard and define the directory for saved summary files \n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter(\"/tmp/ladder\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session() \n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (3000, 784)\n",
      "convertToOneHot(y_train, 10).shape: (3000, 10)\n"
     ]
    }
   ],
   "source": [
    "# check the shape of the feeds:\n",
    "#x = tf.placeholder(tf.float32, shape=[None, 784], name='x_data')\n",
    "#y_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_data')\n",
    "print(\"x_train.shape:\", x_train.shape)  \n",
    "print(\"convertToOneHot(y_train, 10).shape:\", convertToOneHot(y_train, 10).shape)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Training: loss 2.319735050201416 acc 0.046875 Validation: loss 2.3231749534606934 acc 0.082\n",
      "100 Training: loss 1.7501676082611084 acc 0.6875 Validation: loss 1.7814929485321045 acc 0.65\n",
      "200 Training: loss 1.2755136489868164 acc 0.703125 Validation: loss 1.2843124866485596 acc 0.71\n",
      "300 Training: loss 0.8503386974334717 acc 0.859375 Validation: loss 0.9666263461112976 acc 0.8\n",
      "400 Training: loss 0.6365038752555847 acc 0.90625 Validation: loss 0.7514290809631348 acc 0.857\n",
      "500 Training: loss 0.459953635931015 acc 0.9375 Validation: loss 0.6060840487480164 acc 0.882\n",
      "600 Training: loss 0.41558706760406494 acc 0.953125 Validation: loss 0.5162840485572815 acc 0.892\n",
      "700 Training: loss 0.35481980443000793 acc 0.9296875 Validation: loss 0.4534491300582886 acc 0.902\n",
      "800 Training: loss 0.27904239296913147 acc 0.96875 Validation: loss 0.410434365272522 acc 0.906\n",
      "900 Training: loss 0.2426457405090332 acc 0.953125 Validation: loss 0.38147205114364624 acc 0.906\n",
      "1000 Training: loss 0.15311047434806824 acc 0.9921875 Validation: loss 0.3571433126926422 acc 0.905\n",
      "1100 Training: loss 0.20176590979099274 acc 0.9765625 Validation: loss 0.34264665842056274 acc 0.91\n",
      "1200 Training: loss 0.1481855809688568 acc 0.96875 Validation: loss 0.32947424054145813 acc 0.913\n",
      "1300 Training: loss 0.11758215725421906 acc 0.984375 Validation: loss 0.31988412141799927 acc 0.916\n",
      "1400 Training: loss 0.12879925966262817 acc 0.9765625 Validation: loss 0.311586856842041 acc 0.914\n",
      "1500 Training: loss 0.1046377569437027 acc 1.0 Validation: loss 0.3057796061038971 acc 0.913\n",
      "1600 Training: loss 0.07366295903921127 acc 1.0 Validation: loss 0.2989040017127991 acc 0.916\n",
      "1700 Training: loss 0.0816473439335823 acc 0.9921875 Validation: loss 0.2953767478466034 acc 0.917\n",
      "1800 Training: loss 0.07519110292196274 acc 0.9921875 Validation: loss 0.29221901297569275 acc 0.914\n",
      "1900 Training: loss 0.06493522226810455 acc 1.0 Validation: loss 0.28820621967315674 acc 0.914\n",
      "2000 Training: loss 0.06660470366477966 acc 0.9921875 Validation: loss 0.28602737188339233 acc 0.916\n",
      "2100 Training: loss 0.08896785974502563 acc 0.9921875 Validation: loss 0.2851274013519287 acc 0.915\n",
      "2200 Training: loss 0.05484893545508385 acc 1.0 Validation: loss 0.2819841206073761 acc 0.92\n",
      "2300 Training: loss 0.08480518311262131 acc 0.9921875 Validation: loss 0.28046008944511414 acc 0.918\n",
      "2400 Training: loss 0.039123229682445526 acc 1.0 Validation: loss 0.279066801071167 acc 0.917\n",
      "2500 Training: loss 0.03984395042061806 acc 1.0 Validation: loss 0.2794435918331146 acc 0.916\n",
      "2600 Training: loss 0.04552559554576874 acc 0.9921875 Validation: loss 0.2803354263305664 acc 0.915\n",
      "2700 Training: loss 0.041997868567705154 acc 1.0 Validation: loss 0.27827078104019165 acc 0.918\n",
      "2800 Training: loss 0.03283614665269852 acc 1.0 Validation: loss 0.27719762921333313 acc 0.918\n",
      "2900 Training: loss 0.027372730895876884 acc 1.0 Validation: loss 0.2771570384502411 acc 0.92\n",
      "3000 Training: loss 0.03502997010946274 acc 1.0 Validation: loss 0.2758040428161621 acc 0.922\n",
      "3100 Training: loss 0.023569311946630478 acc 1.0 Validation: loss 0.2740927040576935 acc 0.921\n",
      "3200 Training: loss 0.023489410057663918 acc 1.0 Validation: loss 0.2763039767742157 acc 0.919\n",
      "3300 Training: loss 0.022878069430589676 acc 1.0 Validation: loss 0.27387699484825134 acc 0.921\n",
      "3400 Training: loss 0.02006661891937256 acc 1.0 Validation: loss 0.2730223536491394 acc 0.921\n",
      "3500 Training: loss 0.03013456240296364 acc 1.0 Validation: loss 0.2727722227573395 acc 0.92\n",
      "3600 Training: loss 0.025790520012378693 acc 1.0 Validation: loss 0.2720850706100464 acc 0.921\n",
      "3700 Training: loss 0.020086459815502167 acc 1.0 Validation: loss 0.2720319926738739 acc 0.922\n",
      "3800 Training: loss 0.01621883362531662 acc 1.0 Validation: loss 0.27127888798713684 acc 0.921\n",
      "3900 Training: loss 0.025776879861950874 acc 1.0 Validation: loss 0.2716684639453888 acc 0.92\n",
      "4000 Training: loss 0.016785163432359695 acc 1.0 Validation: loss 0.2709796130657196 acc 0.92\n",
      "4100 Training: loss 0.01997179538011551 acc 1.0 Validation: loss 0.27053698897361755 acc 0.919\n",
      "4200 Training: loss 0.018146205693483353 acc 1.0 Validation: loss 0.27138251066207886 acc 0.919\n",
      "4300 Training: loss 0.01463589258491993 acc 1.0 Validation: loss 0.2709049880504608 acc 0.919\n",
      "4400 Training: loss 0.015839172527194023 acc 1.0 Validation: loss 0.27002274990081787 acc 0.92\n",
      "4500 Training: loss 0.017571764066815376 acc 1.0 Validation: loss 0.26927104592323303 acc 0.92\n",
      "4600 Training: loss 0.013190506026148796 acc 1.0 Validation: loss 0.269906222820282 acc 0.92\n",
      "4700 Training: loss 0.013127205893397331 acc 1.0 Validation: loss 0.27012526988983154 acc 0.919\n",
      "4800 Training: loss 0.013562140986323357 acc 1.0 Validation: loss 0.2696211636066437 acc 0.921\n",
      "4900 Training: loss 0.01833130419254303 acc 1.0 Validation: loss 0.2702154815196991 acc 0.92\n"
     ]
    }
   ],
   "source": [
    "vals = []\n",
    "for i in range(5000):\n",
    "    idx = np.random.permutation(len(x_train))[0:128] #Easy minibatch of size 128\n",
    "    loss_, _, res_ = sess.run((loss_supervised, train_op, out), \n",
    "                              feed_dict={x:x_train[idx], y_true:convertToOneHot(y_train[idx], 10)})\n",
    "    if (i % 100 == 0):#50\n",
    "        acc = np.average(np.argmax(res_, axis = 1) == y_train[idx])\n",
    "        # Get the results for the validation results \n",
    "        loss_v, res_val, summary_ = sess.run([loss_supervised, out, merged_summary_op], \n",
    "                                                          feed_dict={x:x_test, \n",
    "                                                                     y_true:convertToOneHot(y_test, 10)})\n",
    "        summary_writer.add_summary(summary_, i)\n",
    "        acc_v = np.average(np.argmax(res_val, axis = 1) == y_test)\n",
    "        vals.append([loss_, acc, loss_v, acc_v])\n",
    "        print(\"{} Training: loss {} acc {} Validation: loss {} acc {}\".format(i, loss_, acc, loss_v, acc_v))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# compare to directory in tf.summary.FileWriter\n",
    "! tensorboard --logdir /tmp/ladder/\n",
    "# check docker call and go to http://srv-lab-t-697:8711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
