{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## pure tf for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# if working on laptop on local docker:\\ndocker run -p 4242:8888 -v ~/dl_cas/:/notebooks -p 6006:6006 -it oduerr/tf_docker:tf1_py3\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# if working on laptop on local docker:\n",
    "docker run -p 4242:8888 -v ~/dl_cas/:/notebooks -p 6006:6006 -it oduerr/tf_docker:tf1_py3\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# python module imports needed in customized functions:\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.0.0',\n",
       " sys.version_info(major=3, minor=4, micro=3, releaselevel='final', serial=0))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# additional imports of python modules\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imgplot\n",
    "import time\n",
    "import pandas as pd\n",
    "#tf.set_random_seed(1)\n",
    "#np.random.seed(1)\n",
    "import sys\n",
    "tf.__version__, sys.version_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def my_fc_bn(Ylogits, offset, scope):\n",
    "    with tf.variable_scope(scope) as v_scope:\n",
    "        mean, variance = tf.nn.moments(Ylogits, [0])\n",
    "        m = mean\n",
    "        v = variance\n",
    "        bnepsilon = 1e-8 #A small float number to avoid dividing by 0\n",
    "        Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n",
    "        return Ybn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def convertToOneHot(vector, num_classes=None):\n",
    "    result = np.zeros((len(vector), num_classes), dtype='int32')\n",
    "    result[np.arange(len(vector)), vector] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data read-in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load small external MNIST data set when for working local on windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small data before split X.shape (4000, 784)\n",
      "small data before  y.shape (4000,)\n",
      "small data x_train.shape: (3000, 784)\n",
      "small data y_train.shape: (3000,)\n",
      "small data x_test.shape: (1000, 784)\n",
      "small data y_test.shape: (1000,)\n",
      "num_class: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# upload mnist_4000.pkl.gz which we have used in the DL course to home\n",
    "# To be compatible with python3 and python2\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open('mnist_4000.pkl.gz', 'rb') as f:\n",
    "    if sys.version_info.major > 2:\n",
    "        (X,y) = pickle.load(f, encoding='latin1')\n",
    "    else:\n",
    "        (X,y) = pickle.load(f)\n",
    "PIXELS = len(X[0,0,0,:])\n",
    "\n",
    "# if images are not flatten (like in mnist) we need first to flatten them\n",
    "# now flatten images for fc ladder\n",
    "\n",
    "X = X.reshape([4000, 784])\n",
    "#X = X/255 # is already normalized\n",
    "\n",
    "print(\"small data before split X.shape\", X.shape)\n",
    "print(\"small data before  y.shape\", y.shape) \n",
    "\n",
    "x_train = X[0:3000]\n",
    "y_train = y[0:3000]\n",
    "x_test = X[3000:4000]\n",
    "y_test = y[3000:4000]\n",
    "\n",
    "\n",
    "print(\"small data x_train.shape:\", x_train.shape)\n",
    "print(\"small data y_train.shape:\",y_train.shape)\n",
    "print(\"small data x_test.shape:\",x_test.shape)\n",
    "print(\"small data y_test.shape:\",y_test.shape)\n",
    "\n",
    "num_class= len(np.unique(y))\n",
    "print(\"num_class:\",num_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Or load full MNIST dataset directly from internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom keras.datasets import mnist\\n\\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\\n\\n# if images are not flatten (like in mnist) we need first to flatten them\\n# now flatten images for fc ladder\\n\\nx_train = x_train.reshape(-1,784)\\nx_test = x_test.reshape(-1,784)\\n\\nprint(\"large data x_train.shape:\", x_train.shape)\\nprint(\"large data y_train.shape:\",y_train.shape)\\nprint(\"large data x_test.shape:\",x_test.shape)\\nprint(\"large data x_test.shape:\",y_test.shape)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# if images are not flatten (like in mnist) we need first to flatten them\n",
    "# now flatten images for fc ladder\n",
    "\n",
    "x_train = x_train.reshape(-1,784)\n",
    "x_test = x_test.reshape(-1,784)\n",
    "\n",
    "print(\"large data x_train.shape:\", x_train.shape)\n",
    "print(\"large data y_train.shape:\",y_train.shape)\n",
    "print(\"large data x_test.shape:\",x_test.shape)\n",
    "print(\"large data x_test.shape:\",y_test.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:200].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0042952602"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(x_train[:,200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83271211"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(x_train[:,200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lets construct a fc NN (784->500->50->10) without noise and unsupervised task to get a benchmark for the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# reset the default graph\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define placeholder which we need later to feed in our data:\n",
    "# be sure that input data is normalized\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784], name='x_data')\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# our benchmark model hast 3 hidden layers\n",
    "# x:h0:784 -> h1:500 -> h2:50 -> h3:10 (softmax)\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform(shape=[784, 500],minval=-0.05, maxval=0.05))  \n",
    "Blt1 = tf.Variable(tf.zeros([500]))\n",
    "B1 = tf.Variable(tf.zeros([500]))\n",
    "W2 = tf.Variable(tf.random_uniform(shape=[500, 50],minval=-0.05, maxval=0.05))\n",
    "Blt2 = tf.Variable(tf.zeros([50]))\n",
    "B2 = tf.Variable(tf.zeros([50]))\n",
    "W3 = tf.Variable(tf.random_uniform(shape=[50, 10],minval=-0.05, maxval=0.05))\n",
    "Blt3 = tf.Variable(tf.zeros([10]))\n",
    "B3 = tf.Variable(tf.zeros([10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define the model architecture as encoder in ladder: \n",
    "\n",
    "#x = my_norm(x, \"initial_z_trafo\")\n",
    "\n",
    "hn_lt_1 = tf.matmul(x, W1) + Blt1 \n",
    "hn_bn_1 = my_fc_bn(Ylogits=hn_lt_1, offset=B1, scope=\"bn\")\n",
    "hn_nlt_1 = tf.nn.relu(hn_bn_1)\n",
    "\n",
    "hn_lt_2 = tf.matmul(hn_nlt_1, W2) + Blt2\n",
    "hn_bn_2 = my_fc_bn(Ylogits=hn_lt_2, offset=B2, scope=\"bn\")\n",
    "hn_nlt_2 = tf.nn.relu(hn_bn_2)\n",
    "\n",
    "hn_lt_3 = tf.matmul(hn_nlt_2, W3) + Blt3\n",
    "hn_bn_3 = hn_lt_3 #my_fc_bn(Ylogits=hn_lt_3, offset=B3, scope=\"bn\")\n",
    "out = tf.nn.softmax(hn_bn_3)  # TODO : IS THIS ERROR PRONE? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# supervised loss\n",
    "############################################################################\n",
    "\n",
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), \n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n",
    "# problems with log(0) which is NaN\n",
    "\n",
    "with tf.name_scope(\"loss_supervised\"):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=hn_bn_3, labels=y_true)\n",
    "    # loss from supervised learning:\n",
    "    loss_supervised = tf.reduce_mean(cross_entropy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train by using  SGD Optimizer\n",
    "with tf.name_scope(\"train_step\"):\n",
    "    train_op = tf.train.AdamOptimizer(0.001).minimize(loss_supervised)\n",
    "    #train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss_supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initialize\"):\n",
    "    init_op = tf.global_variables_initializer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## test dimensions\\n#init_op = tf.global_variables_initializer() \\n# run the graph\\nsess = tf.Session()\\nsess.run(init_op) #initialization on the concrete realization of the graph\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## test dimensions\n",
    "#init_op = tf.global_variables_initializer() \n",
    "# run the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init_op) #initialization on the concrete realization of the graph\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nloss_, _, out_= sess.run(\\n    feed_dict={x:x_train[0:5], y_true:convertToOneHot(y_train[0:5], 10)},\\n    fetches=(loss_supervised, train_op, out)) \\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "loss_, _, out_= sess.run(\n",
    "    feed_dict={x:x_train[0:5], y_true:convertToOneHot(y_train[0:5], 10)},\n",
    "    fetches=(loss_supervised, train_op, out)) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding  Variable:0\n",
      "INFO:tensorflow:Summary name Variable:0 is illegal; using Variable_0 instead.\n",
      "Adding  Variable_1:0\n",
      "INFO:tensorflow:Summary name Variable_1:0 is illegal; using Variable_1_0 instead.\n",
      "Adding  Variable_2:0\n",
      "INFO:tensorflow:Summary name Variable_2:0 is illegal; using Variable_2_0 instead.\n",
      "Adding  Variable_3:0\n",
      "INFO:tensorflow:Summary name Variable_3:0 is illegal; using Variable_3_0 instead.\n",
      "Adding  Variable_4:0\n",
      "INFO:tensorflow:Summary name Variable_4:0 is illegal; using Variable_4_0 instead.\n",
      "Adding  Variable_5:0\n",
      "INFO:tensorflow:Summary name Variable_5:0 is illegal; using Variable_5_0 instead.\n",
      "Adding  Variable_6:0\n",
      "INFO:tensorflow:Summary name Variable_6:0 is illegal; using Variable_6_0 instead.\n",
      "Adding  Variable_7:0\n",
      "INFO:tensorflow:Summary name Variable_7:0 is illegal; using Variable_7_0 instead.\n",
      "Adding  Variable_8:0\n",
      "INFO:tensorflow:Summary name Variable_8:0 is illegal; using Variable_8_0 instead.\n"
     ]
    }
   ],
   "source": [
    "#We want to visualize the development of the following variables in tensorboard:\n",
    "for v in tf.trainable_variables():\n",
    "    print(\"Adding \", v.name)\n",
    "    tf.summary.histogram(v.name, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_supervised_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to visualize the development of the loss in tensorboard\n",
    "\n",
    "tf.summary.scalar(\"loss_supervised\", loss_supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! rm -rf /tmp/ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! mkdir /tmp/ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! ls /tmp/ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# collect all summaries for tensorboard and define the directory for saved summary files \n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter(\"/tmp/ladder\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session() \n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (3000, 784)\n",
      "convertToOneHot(y_train, 10).shape: (3000, 10)\n"
     ]
    }
   ],
   "source": [
    "# check the shape of the feeds:\n",
    "#x = tf.placeholder(tf.float32, shape=[None, 784], name='x_data')\n",
    "#y_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_data')\n",
    "print(\"x_train.shape:\", x_train.shape)  \n",
    "print(\"convertToOneHot(y_train, 10).shape:\", convertToOneHot(y_train, 10).shape)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2044 1399  163 ..., 2292 2104  929]\n",
      "[1274   91  298  541 2235 2774 1100  403 2406 2558 2469 2434  343  515 1240\n",
      "  131 1795 2388 2950 1591  655 1986 1627 1383 2463 2302 1980 2787 1521 2876\n",
      " 2454 1997 2658  720 1837 2703 1604 2224  375 2800 2455 1427 1831 2139 2790\n",
      " 1982  241  775  445 1388 2503 1897  822 2292 2104  929]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2454,  822, 2790, 2434, 1837,  403,  241, 1831, 2455,  343,   91,\n",
       "       2800, 1627,  298, 1795, 1240, 1604, 2703,  929, 2139, 1982, 1100,\n",
       "       2774, 1274, 2224,  131, 2235, 1427, 2503, 2104, 2787, 2469,  445,\n",
       "       2406, 2388,  655, 2463,  720, 2658, 1986, 2302,  775,  541, 2558,\n",
       "       1897, 1383, 2876, 2950,  375, 1980, 1997,  515, 1388, 1591, 1521,\n",
       "       2292])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.permutation(len(x_train)) #Easy minibatch of size 128\n",
    "print(idx)\n",
    "i=23\n",
    "print(idx[i*128:(i*128)+128])\n",
    "np.random.permutation(idx[i*128:(i*128)+128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Training: loss 2.2976877689361572 acc 0.09375 Validation: loss 2.1916520595550537 acc 0.331\n",
      "23 Training: loss 1.418180227279663 acc 0.8515625 Validation: loss 1.4644418954849243 acc 0.801\n",
      "46 Training: loss 1.0111674070358276 acc 0.9375 Validation: loss 1.0530914068222046 acc 0.887\n",
      "69 Training: loss 0.668674647808075 acc 0.9453125 Validation: loss 0.7644112706184387 acc 0.899\n",
      "92 Training: loss 0.41415250301361084 acc 0.9765625 Validation: loss 0.577830970287323 acc 0.913\n",
      "115 Training: loss 0.3388984799385071 acc 0.9765625 Validation: loss 0.4574737250804901 acc 0.928\n",
      "138 Training: loss 0.22033379971981049 acc 0.984375 Validation: loss 0.3915722668170929 acc 0.931\n",
      "161 Training: loss 0.163203626871109 acc 0.984375 Validation: loss 0.343047559261322 acc 0.935\n",
      "184 Training: loss 0.10326725244522095 acc 1.0 Validation: loss 0.32037463784217834 acc 0.938\n",
      "207 Training: loss 0.08463031053543091 acc 1.0 Validation: loss 0.29036402702331543 acc 0.934\n",
      "230 Training: loss 0.08511602878570557 acc 1.0 Validation: loss 0.27038025856018066 acc 0.945\n"
     ]
    }
   ],
   "source": [
    "vals = []\n",
    "for i in range(240):\n",
    "    idx = np.random.permutation(len(x_train))[0:128] #Easy minibatch of size 128\n",
    "    #print(idx[0])\n",
    "    loss_, _, res_ = sess.run((loss_supervised, train_op, out), \n",
    "                              feed_dict={x:x_train[idx], y_true:convertToOneHot(y_train[idx], 10)})\n",
    "    if (i % 23 == 0):#50\n",
    "        acc = np.average(np.argmax(res_, axis = 1) == y_train[idx])\n",
    "        # Get the results for the validation results \n",
    "        loss_v, res_val, summary_ = sess.run([loss_supervised, out, merged_summary_op], \n",
    "                                                          feed_dict={x:x_test, \n",
    "                                                                     y_true:convertToOneHot(y_test, 10)})\n",
    "        summary_writer.add_summary(summary_, i)\n",
    "        acc_v = np.average(np.argmax(res_val, axis = 1) == y_test)\n",
    "        vals.append([loss_, acc, loss_v, acc_v])\n",
    "        print(\"{} Training: loss {} acc {} Validation: loss {} acc {}\".format(i, loss_, acc, loss_v, acc_v))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vals = []\n",
    "idx = np.random.permutation(len(x_train))\n",
    "for j in range(0,10):\n",
    "    for i in range(23):\n",
    "        idx1 = np.random.permutation(idx[i*128:(i*128)+128])\n",
    "        loss_, _, res_ = sess.run((loss_supervised, train_op, out), \n",
    "                                  feed_dict={x:x_train[idx1], y_true:convertToOneHot(y_train[idx1], 10)})\n",
    "        if (i % 1 == 0):#50\n",
    "            acc = np.average(np.argmax(res_, axis = 1) == y_train[idx1])\n",
    "            # Get the results for the validation results \n",
    "            loss_v, res_val, summary_ = sess.run([loss_supervised, out, merged_summary_op], \n",
    "                                                              feed_dict={x:x_test, \n",
    "                                                                         y_true:convertToOneHot(y_test, 10)})\n",
    "            summary_writer.add_summary(summary_, i)\n",
    "            acc_v = np.average(np.argmax(res_val, axis = 1) == y_test)\n",
    "            vals.append([loss_, acc, loss_v, acc_v])\n",
    "            print(\"{} Training: loss {} acc {} Validation: loss {} acc {}\".format(i, loss_, acc, loss_v, acc_v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# compare to directory in tf.summary.FileWriter\n",
    "#! tensorboard --logdir /tmp/ladder/\n",
    "# check docker call and go to http://srv-lab-t-697:8711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
