{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pure tf for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# if working on laptop on local docker:\\ndocker run -p 4242:8888 -v ~/dl_cas/:/notebooks -p 6006:6006 -it oduerr/tf_docker:tf1_py3\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# if working on laptop on local docker:\n",
    "docker run -p 4242:8888 -v ~/dl_cas/:/notebooks -p 6006:6006 -it oduerr/tf_docker:tf1_py3\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# python module imports needed in customized functions:\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.0.0',\n",
       " sys.version_info(major=3, minor=4, micro=3, releaselevel='final', serial=0))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# additional imports of python modules\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imgplot\n",
    "import time\n",
    "import pandas as pd\n",
    "#tf.set_random_seed(1)\n",
    "#np.random.seed(1)\n",
    "import sys\n",
    "tf.__version__, sys.version_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_fc_bn(Ylogits, offset, scope):\n",
    "    with tf.variable_scope(scope) as v_scope:\n",
    "        mean, variance = tf.nn.moments(Ylogits, [0])\n",
    "        m = mean\n",
    "        v = variance\n",
    "        bnepsilon = 1e-8 #A small float number to avoid dividing by 0\n",
    "        Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n",
    "        return Ybn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertToOneHot(vector, num_classes=None):\n",
    "    result = np.zeros((len(vector), num_classes), dtype='int32')\n",
    "    result[np.arange(len(vector)), vector] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data read-in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load small external MNIST data set when for working local on windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small data before split X.shape (4000, 784)\n",
      "small data before  y.shape (4000,)\n",
      "small data x_train.shape: (3000, 784)\n",
      "small data y_train.shape: (3000,)\n",
      "small data x_test.shape: (1000, 784)\n",
      "small data y_test.shape: (1000,)\n",
      "num_class: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# upload mnist_4000.pkl.gz which we have used in the DL course to home\n",
    "# To be compatible with python3 and python2\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open('mnist_4000.pkl.gz', 'rb') as f:\n",
    "    if sys.version_info.major > 2:\n",
    "        (X,y) = pickle.load(f, encoding='latin1')\n",
    "    else:\n",
    "        (X,y) = pickle.load(f)\n",
    "PIXELS = len(X[0,0,0,:])\n",
    "\n",
    "# if images are not flatten (like in mnist) we need first to flatten them\n",
    "# now flatten images for fc ladder\n",
    "\n",
    "X = X.reshape([4000, 784])\n",
    "#X = X/255 # is already normalized\n",
    "\n",
    "print(\"small data before split X.shape\", X.shape)\n",
    "print(\"small data before  y.shape\", y.shape) \n",
    "\n",
    "x_train = X[0:3000]\n",
    "y_train = y[0:3000]\n",
    "x_test = X[3000:4000]\n",
    "y_test = y[3000:4000]\n",
    "\n",
    "\n",
    "print(\"small data x_train.shape:\", x_train.shape)\n",
    "print(\"small data y_train.shape:\",y_train.shape)\n",
    "print(\"small data x_test.shape:\",x_test.shape)\n",
    "print(\"small data y_test.shape:\",y_test.shape)\n",
    "\n",
    "num_class= len(np.unique(y))\n",
    "print(\"num_class:\",num_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Or load full MNIST dataset directly from internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom keras.datasets import mnist\\n\\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\\n\\n# if images are not flatten (like in mnist) we need first to flatten them\\n# now flatten images for fc ladder\\n\\nx_train = x_train.reshape(-1,784)\\nx_test = x_test.reshape(-1,784)\\n\\nprint(\"large data x_train.shape:\", x_train.shape)\\nprint(\"large data y_train.shape:\",y_train.shape)\\nprint(\"large data x_test.shape:\",x_test.shape)\\nprint(\"large data x_test.shape:\",y_test.shape)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# if images are not flatten (like in mnist) we need first to flatten them\n",
    "# now flatten images for fc ladder\n",
    "\n",
    "x_train = x_train.reshape(-1,784)\n",
    "x_test = x_test.reshape(-1,784)\n",
    "\n",
    "print(\"large data x_train.shape:\", x_train.shape)\n",
    "print(\"large data y_train.shape:\",y_train.shape)\n",
    "print(\"large data x_test.shape:\",x_test.shape)\n",
    "print(\"large data x_test.shape:\",y_test.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:200].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0042952602"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(x_train[:,200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83271211"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(x_train[:,200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lets construct a fc NN (784->500->50->10) without noise and unsupervised task to get a benchmark for the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# reset the default graph\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define placeholder which we need later to feed in our data:\n",
    "# be sure that input data is normalized\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784], name='x_data')\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# our benchmark model hast 3 hidden layers\n",
    "# x:h0:784 -> h1:500 -> h2:50 -> h3:10 (softmax)\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform(shape=[784, 500],minval=-0.05, maxval=0.05))  \n",
    "Blt1 = tf.Variable(tf.zeros([500]))\n",
    "B1 = tf.Variable(tf.zeros([500]))\n",
    "W2 = tf.Variable(tf.random_uniform(shape=[500, 50],minval=-0.05, maxval=0.05))\n",
    "Blt2 = tf.Variable(tf.zeros([50]))\n",
    "B2 = tf.Variable(tf.zeros([50]))\n",
    "W3 = tf.Variable(tf.random_uniform(shape=[50, 10],minval=-0.05, maxval=0.05))\n",
    "Blt3 = tf.Variable(tf.zeros([10]))\n",
    "B3 = tf.Variable(tf.zeros([10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define the model architecture as encoder in ladder: \n",
    "\n",
    "#x = my_norm(x, \"initial_z_trafo\")\n",
    "\n",
    "hn_lt_1 = tf.matmul(x, W1) + Blt1 \n",
    "hn_bn_1 = my_fc_bn(Ylogits=hn_lt_1, offset=B1, scope=\"bn\")\n",
    "hn_nlt_1 = tf.nn.relu(hn_bn_1)\n",
    "\n",
    "hn_lt_2 = tf.matmul(hn_nlt_1, W2) + Blt2\n",
    "hn_bn_2 = my_fc_bn(Ylogits=hn_lt_2, offset=B2, scope=\"bn\")\n",
    "hn_nlt_2 = tf.nn.relu(hn_bn_2)\n",
    "\n",
    "hn_lt_3 = tf.matmul(hn_nlt_2, W3) + Blt3\n",
    "hn_bn_3 = hn_lt_3 #my_fc_bn(Ylogits=hn_lt_3, offset=B3, scope=\"bn\")\n",
    "out = tf.nn.softmax(hn_bn_3)  # TODO : IS THIS ERROR PRONE? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# supervised loss\n",
    "############################################################################\n",
    "\n",
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), \n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n",
    "# problems with log(0) which is NaN\n",
    "\n",
    "with tf.name_scope(\"loss_supervised\"):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=hn_bn_3, labels=y_true)\n",
    "    # loss from supervised learning:\n",
    "    loss_supervised = tf.reduce_mean(cross_entropy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initialize\"):\n",
    "    init_op = tf.global_variables_initializer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train by using  SGD Optimizer\n",
    "with tf.name_scope(\"train_step\"):\n",
    "    #train_op = tf.train.AdamOptimizer(0.001).minimize(loss_supervised)\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss_supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## test dimensions\\n#init_op = tf.global_variables_initializer() \\n# run the graph\\nsess = tf.Session()\\nsess.run(init_op) #initialization on the concrete realization of the graph\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## test dimensions\n",
    "#init_op = tf.global_variables_initializer() \n",
    "# run the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init_op) #initialization on the concrete realization of the graph\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nloss_, _, out_= sess.run(\\n    feed_dict={x:x_train[0:5], y_true:convertToOneHot(y_train[0:5], 10)},\\n    fetches=(loss_supervised, train_op, out)) \\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "loss_, _, out_= sess.run(\n",
    "    feed_dict={x:x_train[0:5], y_true:convertToOneHot(y_train[0:5], 10)},\n",
    "    fetches=(loss_supervised, train_op, out)) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding  Variable:0\n",
      "INFO:tensorflow:Summary name Variable:0 is illegal; using Variable_0 instead.\n",
      "Adding  Variable_1:0\n",
      "INFO:tensorflow:Summary name Variable_1:0 is illegal; using Variable_1_0 instead.\n",
      "Adding  Variable_2:0\n",
      "INFO:tensorflow:Summary name Variable_2:0 is illegal; using Variable_2_0 instead.\n",
      "Adding  Variable_3:0\n",
      "INFO:tensorflow:Summary name Variable_3:0 is illegal; using Variable_3_0 instead.\n",
      "Adding  Variable_4:0\n",
      "INFO:tensorflow:Summary name Variable_4:0 is illegal; using Variable_4_0 instead.\n",
      "Adding  Variable_5:0\n",
      "INFO:tensorflow:Summary name Variable_5:0 is illegal; using Variable_5_0 instead.\n",
      "Adding  Variable_6:0\n",
      "INFO:tensorflow:Summary name Variable_6:0 is illegal; using Variable_6_0 instead.\n",
      "Adding  Variable_7:0\n",
      "INFO:tensorflow:Summary name Variable_7:0 is illegal; using Variable_7_0 instead.\n",
      "Adding  Variable_8:0\n",
      "INFO:tensorflow:Summary name Variable_8:0 is illegal; using Variable_8_0 instead.\n"
     ]
    }
   ],
   "source": [
    "#We want to visualize the development of the following variables in tensorboard:\n",
    "for v in tf.trainable_variables():\n",
    "    print(\"Adding \", v.name)\n",
    "    tf.summary.histogram(v.name, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_supervised_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to visualize the development of the loss in tensorboard\n",
    "\n",
    "tf.summary.scalar(\"loss_supervised\", loss_supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! rm -rf /tmp/ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! mkdir /tmp/ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! ls /tmp/ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# collect all summaries for tensorboard and define the directory for saved summary files \n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter(\"/tmp/ladder\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session() \n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (3000, 784)\n",
      "convertToOneHot(y_train, 10).shape: (3000, 10)\n"
     ]
    }
   ],
   "source": [
    "# check the shape of the feeds:\n",
    "#x = tf.placeholder(tf.float32, shape=[None, 784], name='x_data')\n",
    "#y_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_data')\n",
    "print(\"x_train.shape:\", x_train.shape)  \n",
    "print(\"convertToOneHot(y_train, 10).shape:\", convertToOneHot(y_train, 10).shape)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Training: loss 2.3339920043945312 acc 0.0703125 Validation: loss 2.3193774223327637 acc 0.08\n",
      "1 Training: loss 2.3128790855407715 acc 0.1015625 Validation: loss 2.3134565353393555 acc 0.086\n",
      "2 Training: loss 2.313842535018921 acc 0.0859375 Validation: loss 2.3077828884124756 acc 0.092\n",
      "3 Training: loss 2.328007221221924 acc 0.0625 Validation: loss 2.3025190830230713 acc 0.098\n",
      "4 Training: loss 2.3062968254089355 acc 0.09375 Validation: loss 2.297375440597534 acc 0.104\n",
      "5 Training: loss 2.2976977825164795 acc 0.1015625 Validation: loss 2.2919349670410156 acc 0.118\n",
      "6 Training: loss 2.3095879554748535 acc 0.078125 Validation: loss 2.2874443531036377 acc 0.126\n",
      "7 Training: loss 2.2870492935180664 acc 0.125 Validation: loss 2.282801628112793 acc 0.134\n",
      "8 Training: loss 2.2657928466796875 acc 0.1796875 Validation: loss 2.277101516723633 acc 0.152\n",
      "9 Training: loss 2.2916653156280518 acc 0.125 Validation: loss 2.2719051837921143 acc 0.166\n",
      "10 Training: loss 2.277186155319214 acc 0.1328125 Validation: loss 2.2666289806365967 acc 0.184\n",
      "11 Training: loss 2.2700304985046387 acc 0.140625 Validation: loss 2.261606454849243 acc 0.19\n",
      "12 Training: loss 2.2389960289001465 acc 0.203125 Validation: loss 2.25675630569458 acc 0.201\n",
      "13 Training: loss 2.2393012046813965 acc 0.234375 Validation: loss 2.2516613006591797 acc 0.209\n",
      "14 Training: loss 2.2373225688934326 acc 0.21875 Validation: loss 2.2464282512664795 acc 0.232\n",
      "15 Training: loss 2.2526040077209473 acc 0.2109375 Validation: loss 2.241670846939087 acc 0.244\n",
      "16 Training: loss 2.2375404834747314 acc 0.2421875 Validation: loss 2.236542224884033 acc 0.254\n",
      "17 Training: loss 2.2295947074890137 acc 0.296875 Validation: loss 2.231551170349121 acc 0.263\n",
      "18 Training: loss 2.2174246311187744 acc 0.328125 Validation: loss 2.226101875305176 acc 0.277\n",
      "19 Training: loss 2.183514356613159 acc 0.3984375 Validation: loss 2.220245361328125 acc 0.295\n",
      "20 Training: loss 2.2375564575195312 acc 0.28125 Validation: loss 2.2149975299835205 acc 0.306\n",
      "21 Training: loss 2.1973729133605957 acc 0.3359375 Validation: loss 2.209920883178711 acc 0.325\n",
      "22 Training: loss 2.211684465408325 acc 0.3515625 Validation: loss 2.204850435256958 acc 0.334\n",
      "23 Training: loss 2.2079648971557617 acc 0.328125 Validation: loss 2.1993119716644287 acc 0.343\n",
      "24 Training: loss 2.194071054458618 acc 0.328125 Validation: loss 2.194340467453003 acc 0.354\n",
      "25 Training: loss 2.190992832183838 acc 0.4140625 Validation: loss 2.189221143722534 acc 0.362\n",
      "26 Training: loss 2.203338623046875 acc 0.34375 Validation: loss 2.184288263320923 acc 0.368\n",
      "27 Training: loss 2.2060699462890625 acc 0.28125 Validation: loss 2.179100751876831 acc 0.376\n",
      "28 Training: loss 2.139078378677368 acc 0.4140625 Validation: loss 2.173038959503174 acc 0.38\n",
      "29 Training: loss 2.1309499740600586 acc 0.4375 Validation: loss 2.1674726009368896 acc 0.385\n",
      "30 Training: loss 2.1753249168395996 acc 0.3359375 Validation: loss 2.1622490882873535 acc 0.389\n",
      "31 Training: loss 2.17448091506958 acc 0.3359375 Validation: loss 2.157559394836426 acc 0.399\n",
      "32 Training: loss 2.127366304397583 acc 0.4921875 Validation: loss 2.151813268661499 acc 0.407\n",
      "33 Training: loss 2.1481246948242188 acc 0.453125 Validation: loss 2.1465210914611816 acc 0.413\n",
      "34 Training: loss 2.1484742164611816 acc 0.375 Validation: loss 2.1415820121765137 acc 0.424\n",
      "35 Training: loss 2.1155881881713867 acc 0.4453125 Validation: loss 2.136432409286499 acc 0.432\n",
      "36 Training: loss 2.0996694564819336 acc 0.5234375 Validation: loss 2.130728006362915 acc 0.429\n",
      "37 Training: loss 2.1578893661499023 acc 0.3984375 Validation: loss 2.125562906265259 acc 0.433\n",
      "38 Training: loss 2.1105518341064453 acc 0.5078125 Validation: loss 2.1200873851776123 acc 0.436\n",
      "39 Training: loss 2.079690456390381 acc 0.4765625 Validation: loss 2.114320993423462 acc 0.441\n",
      "40 Training: loss 2.1056294441223145 acc 0.4609375 Validation: loss 2.109379768371582 acc 0.446\n",
      "41 Training: loss 2.1209444999694824 acc 0.40625 Validation: loss 2.104311943054199 acc 0.451\n",
      "42 Training: loss 2.113081455230713 acc 0.4453125 Validation: loss 2.0997636318206787 acc 0.455\n",
      "43 Training: loss 2.0767736434936523 acc 0.4921875 Validation: loss 2.093783140182495 acc 0.461\n",
      "44 Training: loss 2.07515811920166 acc 0.5078125 Validation: loss 2.088153123855591 acc 0.468\n",
      "45 Training: loss 2.079143524169922 acc 0.4453125 Validation: loss 2.0823974609375 acc 0.477\n",
      "46 Training: loss 2.023475170135498 acc 0.5625 Validation: loss 2.0762128829956055 acc 0.478\n",
      "47 Training: loss 2.050086736679077 acc 0.4921875 Validation: loss 2.070239543914795 acc 0.482\n",
      "48 Training: loss 2.103217601776123 acc 0.46875 Validation: loss 2.0657989978790283 acc 0.485\n",
      "49 Training: loss 2.0385806560516357 acc 0.515625 Validation: loss 2.0602681636810303 acc 0.486\n",
      "50 Training: loss 2.0322861671447754 acc 0.546875 Validation: loss 2.054887294769287 acc 0.485\n",
      "51 Training: loss 2.0113320350646973 acc 0.53125 Validation: loss 2.0490024089813232 acc 0.487\n",
      "52 Training: loss 2.032622814178467 acc 0.515625 Validation: loss 2.0435285568237305 acc 0.491\n",
      "53 Training: loss 2.006134510040283 acc 0.5546875 Validation: loss 2.0376784801483154 acc 0.489\n",
      "54 Training: loss 2.045348644256592 acc 0.46875 Validation: loss 2.032426118850708 acc 0.496\n",
      "55 Training: loss 1.9979584217071533 acc 0.53125 Validation: loss 2.0265164375305176 acc 0.5\n",
      "56 Training: loss 2.026989698410034 acc 0.4609375 Validation: loss 2.021482229232788 acc 0.504\n",
      "57 Training: loss 2.0602755546569824 acc 0.421875 Validation: loss 2.016674518585205 acc 0.505\n",
      "58 Training: loss 1.993100643157959 acc 0.4921875 Validation: loss 2.0110957622528076 acc 0.508\n",
      "59 Training: loss 2.0329465866088867 acc 0.4609375 Validation: loss 2.0059738159179688 acc 0.512\n",
      "60 Training: loss 1.9678703546524048 acc 0.5703125 Validation: loss 1.9999260902404785 acc 0.518\n",
      "61 Training: loss 2.002971649169922 acc 0.5234375 Validation: loss 1.9944707155227661 acc 0.519\n",
      "62 Training: loss 2.0071840286254883 acc 0.4609375 Validation: loss 1.9892557859420776 acc 0.524\n",
      "63 Training: loss 1.9523229598999023 acc 0.53125 Validation: loss 1.9832199811935425 acc 0.526\n",
      "64 Training: loss 1.9879661798477173 acc 0.5078125 Validation: loss 1.9777886867523193 acc 0.531\n",
      "65 Training: loss 1.9809861183166504 acc 0.5234375 Validation: loss 1.9722906351089478 acc 0.531\n",
      "66 Training: loss 1.9545598030090332 acc 0.546875 Validation: loss 1.966101050376892 acc 0.53\n",
      "67 Training: loss 1.9432406425476074 acc 0.578125 Validation: loss 1.960357666015625 acc 0.533\n",
      "68 Training: loss 1.952845573425293 acc 0.53125 Validation: loss 1.9549144506454468 acc 0.533\n",
      "69 Training: loss 1.9443135261535645 acc 0.5390625 Validation: loss 1.9492275714874268 acc 0.536\n",
      "70 Training: loss 1.9125540256500244 acc 0.5546875 Validation: loss 1.9433503150939941 acc 0.537\n",
      "71 Training: loss 1.899388313293457 acc 0.5859375 Validation: loss 1.9373704195022583 acc 0.538\n",
      "72 Training: loss 1.9311747550964355 acc 0.5234375 Validation: loss 1.9319579601287842 acc 0.543\n",
      "73 Training: loss 1.9381866455078125 acc 0.5078125 Validation: loss 1.9262664318084717 acc 0.539\n",
      "74 Training: loss 1.931727647781372 acc 0.5703125 Validation: loss 1.9206067323684692 acc 0.544\n",
      "75 Training: loss 1.86590576171875 acc 0.6171875 Validation: loss 1.9148778915405273 acc 0.546\n",
      "76 Training: loss 1.9130847454071045 acc 0.5390625 Validation: loss 1.909177541732788 acc 0.545\n",
      "77 Training: loss 1.885313868522644 acc 0.6171875 Validation: loss 1.9032751321792603 acc 0.545\n",
      "78 Training: loss 1.8726398944854736 acc 0.59375 Validation: loss 1.897364854812622 acc 0.541\n",
      "79 Training: loss 1.893172025680542 acc 0.546875 Validation: loss 1.8914884328842163 acc 0.54\n",
      "80 Training: loss 1.8635571002960205 acc 0.578125 Validation: loss 1.8852629661560059 acc 0.539\n",
      "81 Training: loss 1.8343944549560547 acc 0.5859375 Validation: loss 1.879388451576233 acc 0.54\n",
      "82 Training: loss 1.8961377143859863 acc 0.53125 Validation: loss 1.8738090991973877 acc 0.542\n",
      "83 Training: loss 1.7977638244628906 acc 0.609375 Validation: loss 1.8671648502349854 acc 0.539\n",
      "84 Training: loss 1.8694034814834595 acc 0.546875 Validation: loss 1.8615567684173584 acc 0.542\n",
      "85 Training: loss 1.8628015518188477 acc 0.5546875 Validation: loss 1.8560000658035278 acc 0.546\n",
      "86 Training: loss 1.8657294511795044 acc 0.515625 Validation: loss 1.8503237962722778 acc 0.549\n",
      "87 Training: loss 1.8436386585235596 acc 0.53125 Validation: loss 1.8444228172302246 acc 0.55\n",
      "88 Training: loss 1.8051645755767822 acc 0.640625 Validation: loss 1.8384404182434082 acc 0.552\n",
      "89 Training: loss 1.848475456237793 acc 0.5625 Validation: loss 1.8327699899673462 acc 0.552\n",
      "90 Training: loss 1.786339521408081 acc 0.6015625 Validation: loss 1.8266748189926147 acc 0.553\n",
      "91 Training: loss 1.8029892444610596 acc 0.625 Validation: loss 1.820876121520996 acc 0.553\n",
      "92 Training: loss 1.8255822658538818 acc 0.578125 Validation: loss 1.8152107000350952 acc 0.556\n",
      "93 Training: loss 1.7510186433792114 acc 0.6328125 Validation: loss 1.809257984161377 acc 0.556\n",
      "94 Training: loss 1.80531644821167 acc 0.53125 Validation: loss 1.803600549697876 acc 0.556\n",
      "95 Training: loss 1.7343660593032837 acc 0.59375 Validation: loss 1.797696828842163 acc 0.556\n",
      "96 Training: loss 1.7623108625411987 acc 0.65625 Validation: loss 1.7922585010528564 acc 0.558\n",
      "97 Training: loss 1.7374908924102783 acc 0.609375 Validation: loss 1.7867450714111328 acc 0.556\n",
      "98 Training: loss 1.769775152206421 acc 0.6171875 Validation: loss 1.7810475826263428 acc 0.558\n",
      "99 Training: loss 1.7539483308792114 acc 0.6015625 Validation: loss 1.7753349542617798 acc 0.559\n"
     ]
    }
   ],
   "source": [
    "vals = []\n",
    "for i in range(100):\n",
    "    idx = np.random.permutation(len(x_train))[0:128] #Easy minibatch of size 128\n",
    "    loss_, _, res_ = sess.run((loss_supervised, train_op, out), \n",
    "                              feed_dict={x:x_train[idx], y_true:convertToOneHot(y_train[idx], 10)})\n",
    "    if (i % 1 == 0):#50\n",
    "        acc = np.average(np.argmax(res_, axis = 1) == y_train[idx])\n",
    "        # Get the results for the validation results \n",
    "        loss_v, res_val, summary_ = sess.run([loss_supervised, out, merged_summary_op], \n",
    "                                                          feed_dict={x:x_test, \n",
    "                                                                     y_true:convertToOneHot(y_test, 10)})\n",
    "        summary_writer.add_summary(summary_, i)\n",
    "        acc_v = np.average(np.argmax(res_val, axis = 1) == y_test)\n",
    "        vals.append([loss_, acc, loss_v, acc_v])\n",
    "        print(\"{} Training: loss {} acc {} Validation: loss {} acc {}\".format(i, loss_, acc, loss_v, acc_v))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# compare to directory in tf.summary.FileWriter\n",
    "#! tensorboard --logdir /tmp/ladder/\n",
    "# check docker call and go to http://srv-lab-t-697:8711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
