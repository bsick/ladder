{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pure tf for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# if working on laptop on local docker:\\ndocker run -p 4242:8888 -v ~/dl_cas/:/notebooks -p 6006:6006 -it oduerr/tf_docker:tf1_py3\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# if working on laptop on local docker:\n",
    "nvidia-docker run -p 8710:8888 -v ~/dl_cas/:/notebooks -p 8711:6006 -v /cluster/home/sick/:/notebooks/local -it oduerr/tf_docker:gpu_r\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python module imports needed in customized functions:\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.4.1',\n",
       " sys.version_info(major=3, minor=5, micro=2, releaselevel='final', serial=0))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# additional imports of python modules\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imgplot\n",
    "import time\n",
    "import pandas as pd\n",
    "#tf.set_random_seed(1)\n",
    "#np.random.seed(1)\n",
    "import sys\n",
    "tf.__version__, sys.version_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_fc_bn(Ylogits, offset, scope):\n",
    "    with tf.variable_scope(scope) as v_scope:\n",
    "        mean, variance = tf.nn.moments(Ylogits, [0])\n",
    "        m = mean\n",
    "        v = variance\n",
    "        bnepsilon = 1e-8 #A small float number to avoid dividing by 0\n",
    "        Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n",
    "        return Ybn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToOneHot(vector, num_classes=None):\n",
    "    result = np.zeros((len(vector), num_classes), dtype='int32')\n",
    "    result[np.arange(len(vector)), vector] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data read-in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load small external MNIST data set when for working local on windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small data before split X.shape (4000, 784)\n",
      "small data before  y.shape (4000,)\n",
      "small data x_train.shape: (3000, 784)\n",
      "small data y_train.shape: (3000,)\n",
      "small data x_test.shape: (1000, 784)\n",
      "small data y_test.shape: (1000,)\n",
      "num_class: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# upload mnist_4000.pkl.gz which we have used in the DL course to home\n",
    "# To be compatible with python3 and python2\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open('mnist_4000.pkl.gz', 'rb') as f:\n",
    "    if sys.version_info.major > 2:\n",
    "        (X,y) = pickle.load(f, encoding='latin1')\n",
    "    else:\n",
    "        (X,y) = pickle.load(f)\n",
    "PIXELS = len(X[0,0,0,:])\n",
    "\n",
    "# if images are not flatten (like in mnist) we need first to flatten them\n",
    "# now flatten images for fc ladder\n",
    "\n",
    "X = X.reshape([4000, 784])\n",
    "#X = X/255 # is already normalized\n",
    "\n",
    "print(\"small data before split X.shape\", X.shape)\n",
    "print(\"small data before  y.shape\", y.shape) \n",
    "\n",
    "x_train = X[0:3000]\n",
    "y_train = y[0:3000]\n",
    "x_test = X[3000:4000]\n",
    "y_test = y[3000:4000]\n",
    "\n",
    "\n",
    "print(\"small data x_train.shape:\", x_train.shape)\n",
    "print(\"small data y_train.shape:\",y_train.shape)\n",
    "print(\"small data x_test.shape:\",x_test.shape)\n",
    "print(\"small data y_test.shape:\",y_test.shape)\n",
    "\n",
    "num_class= len(np.unique(y))\n",
    "print(\"num_class:\",num_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Or load full MNIST dataset directly from internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom keras.datasets import mnist\\n\\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\\n\\n# if images are not flatten (like in mnist) we need first to flatten them\\n# now flatten images for fc ladder\\n\\nx_train = x_train.reshape(-1,784)\\nx_test = x_test.reshape(-1,784)\\n\\nprint(\"large data x_train.shape:\", x_train.shape)\\nprint(\"large data y_train.shape:\",y_train.shape)\\nprint(\"large data x_test.shape:\",x_test.shape)\\nprint(\"large data x_test.shape:\",y_test.shape)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# if images are not flatten (like in mnist) we need first to flatten them\n",
    "# now flatten images for fc ladder\n",
    "\n",
    "x_train = x_train.reshape(-1,784)\n",
    "x_test = x_test.reshape(-1,784)\n",
    "\n",
    "print(\"large data x_train.shape:\", x_train.shape)\n",
    "print(\"large data y_train.shape:\",y_train.shape)\n",
    "print(\"large data x_test.shape:\",x_test.shape)\n",
    "print(\"large data x_test.shape:\",y_test.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:200].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0042952602"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(x_train[:,200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83271211"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(x_train[:,200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lets construct a fc NN (784->500->50->10) without noise and unsupervised task to get a benchmark for the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the default graph\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define placeholder which we need later to feed in our data:\n",
    "# be sure that input data is normalized\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784], name='x_data')\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our benchmark model hast 3 hidden layers\n",
    "# x:h0:784 -> h1:500 -> h2:50 -> h3:10 (softmax)\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform(shape=[784, 500],minval=-0.05, maxval=0.05))  \n",
    "Blt1 = tf.Variable(tf.zeros([500]))\n",
    "B1 = tf.Variable(tf.zeros([500]))\n",
    "W2 = tf.Variable(tf.random_uniform(shape=[500, 50],minval=-0.05, maxval=0.05))\n",
    "Blt2 = tf.Variable(tf.zeros([50]))\n",
    "B2 = tf.Variable(tf.zeros([50]))\n",
    "W3 = tf.Variable(tf.random_uniform(shape=[50, 10],minval=-0.05, maxval=0.05))\n",
    "Blt3 = tf.Variable(tf.zeros([10]))\n",
    "B3 = tf.Variable(tf.zeros([10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model architecture as encoder in ladder: \n",
    "\n",
    "#x = my_norm(x, \"initial_z_trafo\")\n",
    "\n",
    "hn_lt_1 = tf.matmul(x, W1) + Blt1 \n",
    "hn_bn_1 = my_fc_bn(Ylogits=hn_lt_1, offset=B1, scope=\"bn\")\n",
    "hn_nlt_1 = tf.nn.relu(hn_bn_1)\n",
    "\n",
    "hn_lt_2 = tf.matmul(hn_nlt_1, W2) + Blt2\n",
    "hn_bn_2 = my_fc_bn(Ylogits=hn_lt_2, offset=B2, scope=\"bn\")\n",
    "hn_nlt_2 = tf.nn.relu(hn_bn_2)\n",
    "\n",
    "hn_lt_3 = tf.matmul(hn_nlt_2, W3) + Blt3\n",
    "hn_bn_3 = hn_lt_3 #my_fc_bn(Ylogits=hn_lt_3, offset=B3, scope=\"bn\")\n",
    "out = tf.nn.softmax(hn_bn_3)  # TODO : IS THIS ERROR PRONE? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# supervised loss\n",
    "############################################################################\n",
    "\n",
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), \n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n",
    "# problems with log(0) which is NaN\n",
    "\n",
    "with tf.name_scope(\"loss_supervised\"):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=hn_bn_3, labels=y_true)\n",
    "    # loss from supervised learning:\n",
    "    loss_supervised = tf.reduce_mean(cross_entropy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Optimizer\n",
    "# if using Adam it is important that the definigion of the train step has \n",
    "# has to appear before the intialization step (since inside adam optimizer variables\n",
    "# are defined which then have to be intialized as well)\n",
    "with tf.name_scope(\"train_step\"):\n",
    "    train_op = tf.train.AdamOptimizer(0.001).minimize(loss_supervised)\n",
    "    #train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss_supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"initialize\"):\n",
    "    init_op = tf.global_variables_initializer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## test dimensions\\n#init_op = tf.global_variables_initializer() \\n# run the graph\\nsess = tf.Session()\\nsess.run(init_op) #initialization on the concrete realization of the graph\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## test dimensions\n",
    "#init_op = tf.global_variables_initializer() \n",
    "# run the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init_op) #initialization on the concrete realization of the graph\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nloss_, _, out_= sess.run(\\n    feed_dict={x:x_train[0:5], y_true:convertToOneHot(y_train[0:5], 10)},\\n    fetches=(loss_supervised, train_op, out)) \\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "loss_, _, out_= sess.run(\n",
    "    feed_dict={x:x_train[0:5], y_true:convertToOneHot(y_train[0:5], 10)},\n",
    "    fetches=(loss_supervised, train_op, out)) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding  Variable:0\n",
      "INFO:tensorflow:Summary name Variable:0 is illegal; using Variable_0 instead.\n",
      "Adding  Variable_1:0\n",
      "INFO:tensorflow:Summary name Variable_1:0 is illegal; using Variable_1_0 instead.\n",
      "Adding  Variable_2:0\n",
      "INFO:tensorflow:Summary name Variable_2:0 is illegal; using Variable_2_0 instead.\n",
      "Adding  Variable_3:0\n",
      "INFO:tensorflow:Summary name Variable_3:0 is illegal; using Variable_3_0 instead.\n",
      "Adding  Variable_4:0\n",
      "INFO:tensorflow:Summary name Variable_4:0 is illegal; using Variable_4_0 instead.\n",
      "Adding  Variable_5:0\n",
      "INFO:tensorflow:Summary name Variable_5:0 is illegal; using Variable_5_0 instead.\n",
      "Adding  Variable_6:0\n",
      "INFO:tensorflow:Summary name Variable_6:0 is illegal; using Variable_6_0 instead.\n",
      "Adding  Variable_7:0\n",
      "INFO:tensorflow:Summary name Variable_7:0 is illegal; using Variable_7_0 instead.\n",
      "Adding  Variable_8:0\n",
      "INFO:tensorflow:Summary name Variable_8:0 is illegal; using Variable_8_0 instead.\n"
     ]
    }
   ],
   "source": [
    "#We want to visualize the development of the following variables in tensorboard:\n",
    "for v in tf.trainable_variables():\n",
    "    print(\"Adding \", v.name)\n",
    "    tf.summary.histogram(v.name, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_supervised_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to visualize the development of the loss in tensorboard\n",
    "\n",
    "tf.summary.scalar(\"loss_supervised\", loss_supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /tmp/ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir /tmp/ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /tmp/ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all summaries for tensorboard and define the directory for saved summary files \n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter(\"/tmp/ladder\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session() \n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (3000, 784)\n",
      "convertToOneHot(y_train, 10).shape: (3000, 10)\n"
     ]
    }
   ],
   "source": [
    "# check the shape of the feeds:\n",
    "#x = tf.placeholder(tf.float32, shape=[None, 784], name='x_data')\n",
    "#y_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_data')\n",
    "print(\"x_train.shape:\", x_train.shape)  \n",
    "print(\"convertToOneHot(y_train, 10).shape:\", convertToOneHot(y_train, 10).shape)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1938  546   96 ..., 2130 1083  509]\n",
      "[1782 1812  746 2242 2597  282 1597 2474 2914  716 1410 2924  276 2111 2386\n",
      " 1436 1923 1445 1240  804 1623 1824 2737 1053 1055  788 2400 1570  106 1748\n",
      " 2373 2985  620 1557 2858  739  328 1501 2056  852 2900 2453 1935   68 1309\n",
      " 1296 2039  209  438 2079  500 2392  401 2130 1083  509]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2400,  620, 2474, 1296,  282,  739,  716,  276,  328, 2924, 1240,\n",
       "       1812, 1597, 2111, 2056,  788, 1782, 2242,  106, 2900, 2985, 2597,\n",
       "        804, 1748, 1824, 2130, 2858, 1501, 1570, 2453, 2039,  401, 1923,\n",
       "         68,  209, 1935,  509, 1083, 2079, 2914, 1445, 1309, 2373, 1410,\n",
       "       2386, 1436,  438, 1053, 2392, 1055, 1623, 2737,  500,  746,  852,\n",
       "       1557])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.permutation(len(x_train)) #Easy minibatch of size 128\n",
    "print(idx)\n",
    "i=23\n",
    "print(idx[i*128:(i*128)+128])\n",
    "np.random.permutation(idx[i*128:(i*128)+128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Training: loss 2.299053192138672 acc 0.1640625 Validation: loss 2.1674246788024902 acc 0.422\n",
      "23 Training: loss 1.3796892166137695 acc 0.8984375 Validation: loss 1.438981294631958 acc 0.851\n",
      "46 Training: loss 0.9846053719520569 acc 0.9453125 Validation: loss 1.0292503833770752 acc 0.904\n",
      "69 Training: loss 0.6669174432754517 acc 0.9609375 Validation: loss 0.7443663477897644 acc 0.911\n",
      "92 Training: loss 0.49896496534347534 acc 0.9296875 Validation: loss 0.5589761734008789 acc 0.922\n",
      "115 Training: loss 0.32968568801879883 acc 0.96875 Validation: loss 0.4491449296474457 acc 0.926\n",
      "138 Training: loss 0.1967659443616867 acc 1.0 Validation: loss 0.381673127412796 acc 0.935\n",
      "161 Training: loss 0.2063540518283844 acc 0.984375 Validation: loss 0.32997456192970276 acc 0.942\n",
      "184 Training: loss 0.11490478366613388 acc 0.9921875 Validation: loss 0.29464325308799744 acc 0.942\n",
      "207 Training: loss 0.08265043795108795 acc 1.0 Validation: loss 0.27058395743370056 acc 0.946\n",
      "230 Training: loss 0.07208776473999023 acc 1.0 Validation: loss 0.2451494336128235 acc 0.943\n"
     ]
    }
   ],
   "source": [
    "vals = []\n",
    "for i in range(240):\n",
    "    idx = np.random.permutation(len(x_train))[0:128] #Easy minibatch of size 128\n",
    "    #print(idx[0])\n",
    "    loss_, _, res_ = sess.run((loss_supervised, train_op, out), \n",
    "                              feed_dict={x:x_train[idx], y_true:convertToOneHot(y_train[idx], 10)})\n",
    "    if (i % 23 == 0):#50\n",
    "        acc = np.average(np.argmax(res_, axis = 1) == y_train[idx])\n",
    "        # Get the results for the validation results \n",
    "        loss_v, res_val, summary_ = sess.run([loss_supervised, out, merged_summary_op], \n",
    "                                                          feed_dict={x:x_test, \n",
    "                                                                     y_true:convertToOneHot(y_test, 10)})\n",
    "        summary_writer.add_summary(summary_, i)\n",
    "        acc_v = np.average(np.argmax(res_val, axis = 1) == y_test)\n",
    "        vals.append([loss_, acc, loss_v, acc_v])\n",
    "        print(\"{} Training: loss {} acc {} Validation: loss {} acc {}\".format(i, loss_, acc, loss_v, acc_v))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vals = []\n",
    "idx = np.random.permutation(len(x_train))\n",
    "for j in range(0,10):\n",
    "    for i in range(23):\n",
    "        idx1 = np.random.permutation(idx[i*128:(i*128)+128])\n",
    "        loss_, _, res_ = sess.run((loss_supervised, train_op, out), \n",
    "                                  feed_dict={x:x_train[idx1], y_true:convertToOneHot(y_train[idx1], 10)})\n",
    "        if (i % 1 == 0):#50\n",
    "            acc = np.average(np.argmax(res_, axis = 1) == y_train[idx1])\n",
    "            # Get the results for the validation results \n",
    "            loss_v, res_val, summary_ = sess.run([loss_supervised, out, merged_summary_op], \n",
    "                                                              feed_dict={x:x_test, \n",
    "                                                                         y_true:convertToOneHot(y_test, 10)})\n",
    "            summary_writer.add_summary(summary_, i)\n",
    "            acc_v = np.average(np.argmax(res_val, axis = 1) == y_test)\n",
    "            vals.append([loss_, acc, loss_v, acc_v])\n",
    "            print(\"{} Training: loss {} acc {} Validation: loss {} acc {}\".format(i, loss_, acc, loss_v, acc_v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare to directory in tf.summary.FileWriter\n",
    "#! tensorboard --logdir /tmp/ladder/\n",
    "# check docker call and go to http://srv-lab-t-697:8711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
